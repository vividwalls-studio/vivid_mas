#!/usr/bin/env python3
"""
Cleanup redundant workflows and create missing ones for VividWalls MAS
Based on data flow definitions
"""

import json
import os
import shutil
from pathlib import Path
from datetime import datetime
import uuid

# Base paths
BASE_PATH = Path("/Volumes/SeagatePortableDrive/Projects/vivid_mas")
N8N_WORKFLOWS_PATH = BASE_PATH / "services/n8n/agents/workflows"
AGENTS_DATA_PATH = BASE_PATH / "services/agents"
ARCHIVE_PATH = BASE_PATH / "services/n8n/agents/workflows/archive"

# Workflows to keep (based on data flow definitions)
REQUIRED_WORKFLOWS = {
    "campaign_agent": ["Campaign Agent", "campaign-agent-workflow"],
    "campaign_manager_agent": ["Campaign Manager Agent Strategic", "campaign-manager-agent"],
    "content_strategy_agent": ["Content Strategy Agent", "content-strategy-workflow"],
    "copy_writer_agent": ["copywriter_workflow", "copy-writer-workflow"],
    "creative_director_agent": ["Creative Director Agent", "creative-director-workflow"],
    "customer_experience_director_agent": ["Customer Experience Director", "customer-experience-director"],
    "data_analytics_agent": ["Data Analytics Agent", "data-analytics-workflow"],
    "email_marketing_agent": ["emailmarketing_workflow", "email-marketing-workflow"],
    "facebook_subagent": ["facebook_marketing_knowledge_gatherer_agent", "facebook-agent"],
    "hospitality_sales_agent": ["hospitalitysales_workflow", "hospitality-sales"],
    "instagram_subagent": ["Instagram Agent", "instagram-agent"],
    "marketing_director_agent": ["Marketing Director Agent", "marketing-director"],
    "marketing_research_agent": ["Marketing Research Agent", "marketing-research"],
    "pinterest_subagent": ["pinterest_workflow", "pinterest-agent"],
    "product_director_agent": ["Product Director Agent", "product-director"],
    "sales_director_agent": ["salesdirector_workflow", "sales-director"],
    "social_media_agent": ["Social Media Director", "social-media-agent"]
}

# Workflows to delete (redundant or obsolete)
WORKFLOWS_TO_DELETE = [
    # Test/Example workflows
    "geographic-income-campaign-workflow",
    "medusa-integration-workflow",
    "medusa-inventory-ai-workflow",
    
    # Duplicates in wrong locations
    "services/agents/workflows/archived",
    "services/agents/workflows/migration_backup_20250719_170823",
    "services/agents/workflows/templates",
    
    # Old format workflows to be replaced
    "lead-generation-workflow",  # duplicate of leadgeneration_workflow
    "copyeditor_workflow",  # should be copy-editor
]

# Utility workflows to keep (not in data flow but needed)
UTILITY_WORKFLOWS = [
    "supabase-to-twenty-lead-sync",
    "twenty-to-supabase-lead-sync",
    "twenty-webhook-handler",
    "n8n_agent_reasoning_workflow",
    "workflow_index",
    "complete-mcp-servers-documentation",
    "mcp-servers-documentation"
]

def create_workflow_template(agent_name, agent_key, department=""):
    """Create a standard n8n workflow template"""
    workflow_id = str(uuid.uuid4())
    
    return {
        "name": f"VividWalls-{agent_name.replace('Agent', '').strip()}-MCP-Agent",
        "nodes": [
            {
                "parameters": {
                    "httpMethod": "POST",
                    "path": f"/{agent_key.replace('_', '-')}",
                    "options": {}
                },
                "id": f"{workflow_id}-webhook",
                "name": "Webhook",
                "type": "n8n-nodes-base.webhook",
                "typeVersion": 1.1,
                "position": [250, 300]
            },
            {
                "parameters": {
                    "model": "gpt-4o",
                    "messages": {
                        "values": [
                            {
                                "content": f"You are the {agent_name} for VividWalls Multi-Agent System.",
                                "role": "system"
                            },
                            {
                                "content": "={{$json.query}}",
                                "role": "user"
                            }
                        ]
                    },
                    "options": {
                        "temperature": 0.7
                    }
                },
                "id": f"{workflow_id}-openai",
                "name": "OpenAI",
                "type": "@n8n/n8n-nodes-langchain.openAi",
                "typeVersion": 1.4,
                "position": [650, 300]
            },
            {
                "parameters": {
                    "operation": "insert",
                    "tableId": "agent_interactions",
                    "fieldsUi": {
                        "fieldValues": [
                            {
                                "fieldName": "agent_name",
                                "fieldValue": f"={agent_name}"
                            },
                            {
                                "fieldName": "interaction_type",
                                "fieldValue": "={{$json.type}}"
                            },
                            {
                                "fieldName": "input",
                                "fieldValue": "={{$json.query}}"
                            },
                            {
                                "fieldName": "output",
                                "fieldValue": "={{$json.content}}"
                            },
                            {
                                "fieldName": "timestamp",
                                "fieldValue": "={{$now}}"
                            }
                        ]
                    }
                },
                "id": f"{workflow_id}-supabase",
                "name": "Log to Supabase",
                "type": "n8n-nodes-base.supabase",
                "typeVersion": 1,
                "position": [850, 300]
            },
            {
                "parameters": {
                    "options": {}
                },
                "id": f"{workflow_id}-respond",
                "name": "Respond to Webhook",
                "type": "n8n-nodes-base.respondToWebhook",
                "typeVersion": 1,
                "position": [1050, 300]
            }
        ],
        "connections": {
            "Webhook": {
                "main": [
                    [
                        {
                            "node": "OpenAI",
                            "type": "main",
                            "index": 0
                        }
                    ]
                ]
            },
            "OpenAI": {
                "main": [
                    [
                        {
                            "node": "Log to Supabase",
                            "type": "main",
                            "index": 0
                        }
                    ]
                ]
            },
            "Log to Supabase": {
                "main": [
                    [
                        {
                            "node": "Respond to Webhook",
                            "type": "main",
                            "index": 0
                        }
                    ]
                ]
            }
        },
        "active": False,
        "settings": {
            "executionOrder": "v1"
        },
        "versionId": workflow_id,
        "meta": {
            "instanceId": workflow_id
        },
        "tags": [
            {
                "name": department or "general",
                "createdAt": datetime.now().isoformat(),
                "updatedAt": datetime.now().isoformat()
            },
            {
                "name": "mcp-agent",
                "createdAt": datetime.now().isoformat(),
                "updatedAt": datetime.now().isoformat()
            }
        ]
    }

def archive_workflows():
    """Archive redundant and obsolete workflows"""
    print("\nüì¶ Archiving redundant workflows...")
    
    # Create archive directory with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    archive_dir = ARCHIVE_PATH / f"cleanup_{timestamp}"
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    archived_count = 0
    
    # Archive old directories
    old_dirs = [
        BASE_PATH / "services/agents/workflows/archived",
        BASE_PATH / "services/agents/workflows/migration_backup_20250719_170823",
        BASE_PATH / "services/agents/workflows/templates"
    ]
    
    for old_dir in old_dirs:
        if old_dir.exists():
            dest = archive_dir / old_dir.name
            print(f"  Archiving directory: {old_dir.name}")
            shutil.move(str(old_dir), str(dest))
            archived_count += 1
    
    # Archive specific obsolete workflows
    for workflow_name in WORKFLOWS_TO_DELETE:
        if "/" not in workflow_name:  # Single file name
            # Find and archive the file
            for workflow_file in N8N_WORKFLOWS_PATH.rglob(f"{workflow_name}.json"):
                dest = archive_dir / workflow_file.name
                print(f"  Archiving: {workflow_file.name}")
                shutil.move(str(workflow_file), str(dest))
                archived_count += 1
    
    print(f"‚úÖ Archived {archived_count} items to {archive_dir}")
    return archive_dir

def consolidate_duplicates():
    """Consolidate duplicate workflows, keeping the most complete version"""
    print("\nüîÑ Consolidating duplicate workflows...")
    
    consolidation_map = {
        "copywriter_workflow": "copywriter_workflow",  # Keep as is
        "copyeditor_workflow": None,  # Delete, will create copy-editor
        "leadgeneration_workflow": "leadgeneration_workflow",  # Keep as is
        "lead-generation-workflow": None,  # Delete duplicate
    }
    
    consolidated_count = 0
    
    for workflow, action in consolidation_map.items():
        if action is None:
            # Delete this workflow
            for workflow_file in N8N_WORKFLOWS_PATH.rglob(f"{workflow}.json"):
                print(f"  Removing duplicate: {workflow_file.name}")
                workflow_file.unlink()
                consolidated_count += 1
    
    print(f"‚úÖ Consolidated {consolidated_count} duplicate workflows")

def create_missing_workflows():
    """Create missing workflows based on data flow definitions"""
    print("\n‚ûï Creating missing workflows...")
    
    created_count = 0
    
    # Check for truly missing workflows
    missing_agents = {
        "campaign_agent": {
            "name": "Campaign Agent",
            "department": "marketing"
        },
        "content_strategy_agent": {
            "name": "Content Strategy Agent",
            "department": "marketing"
        },
        "creative_director_agent": {
            "name": "Creative Director Agent",
            "department": "marketing"
        },
        "copy_editor_agent": {
            "name": "Copy Editor Agent",
            "department": "marketing"
        },
        "customer_experience_director_agent": {
            "name": "Customer Experience Director Agent",
            "department": "customer_experience"
        },
        "data_analytics_agent": {
            "name": "Data Analytics Agent",
            "department": "analytics"
        },
        "instagram_subagent": {
            "name": "Instagram Agent",
            "department": "social_media"
        },
        "marketing_director_agent": {
            "name": "Marketing Director Agent",
            "department": "marketing"
        },
        "marketing_research_agent": {
            "name": "Marketing Research Agent",
            "department": "marketing"
        },
        "product_director_agent": {
            "name": "Product Director Agent",
            "department": "product"
        }
    }
    
    for agent_key, agent_info in missing_agents.items():
        # Check if workflow already exists with alternate name
        existing = False
        for alt_name in REQUIRED_WORKFLOWS.get(agent_key, []):
            if any(N8N_WORKFLOWS_PATH.rglob(f"*{alt_name}*.json")):
                existing = True
                break
        
        if not existing:
            # Create the workflow
            workflow = create_workflow_template(
                agent_info["name"], 
                agent_key,
                agent_info["department"]
            )
            
            # Determine output path
            dept_dir = N8N_WORKFLOWS_PATH / agent_info["department"]
            dept_dir.mkdir(parents=True, exist_ok=True)
            
            output_file = dept_dir / f"{agent_key.replace('_', '')}_workflow.json"
            
            with open(output_file, 'w') as f:
                json.dump(workflow, f, indent=2)
            
            print(f"  Created: {output_file.name}")
            created_count += 1
    
    print(f"‚úÖ Created {created_count} new workflows")

def organize_workflows():
    """Organize workflows into proper department directories"""
    print("\nüìÅ Organizing workflows by department...")
    
    department_map = {
        "sales": ["sales", "lead", "partnership", "account"],
        "marketing": ["marketing", "email", "newsletter", "copy", "keyword", "campaign", "content"],
        "social_media": ["pinterest", "facebook", "instagram", "twitter", "linkedin", "tiktok", "youtube"],
        "finance": ["accounting", "budget", "financial"],
        "operations": ["inventory", "supply", "quality", "logistics", "vendor"],
        "product": ["product", "catalog"],
        "customer_experience": ["customer", "support", "livechat"],
        "analytics": ["analytics", "data"]
    }
    
    moved_count = 0
    
    # Find workflows in root and move to appropriate departments
    root_workflows = list(N8N_WORKFLOWS_PATH.glob("*.json"))
    
    for workflow_file in root_workflows:
        workflow_name = workflow_file.stem.lower()
        
        # Skip utility workflows
        if workflow_name in [w.lower() for w in UTILITY_WORKFLOWS]:
            continue
        
        # Find appropriate department
        for dept, keywords in department_map.items():
            if any(keyword in workflow_name for keyword in keywords):
                dept_dir = N8N_WORKFLOWS_PATH / dept
                dept_dir.mkdir(parents=True, exist_ok=True)
                
                dest = dept_dir / workflow_file.name
                print(f"  Moving {workflow_file.name} ‚Üí {dept}/")
                shutil.move(str(workflow_file), str(dest))
                moved_count += 1
                break
    
    print(f"‚úÖ Organized {moved_count} workflows")

def generate_summary_report():
    """Generate a summary report of all changes"""
    print("\nüìä Generating summary report...")
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "actions_taken": {
            "archived_workflows": [],
            "created_workflows": [],
            "organized_workflows": [],
            "remaining_workflows": []
        },
        "statistics": {
            "total_before": 0,
            "total_after": 0,
            "archived": 0,
            "created": 0,
            "organized": 0
        }
    }
    
    # Count final workflows
    for dept_dir in N8N_WORKFLOWS_PATH.iterdir():
        if dept_dir.is_dir() and not dept_dir.name.startswith('.'):
            dept_workflows = list(dept_dir.glob("*.json"))
            report["actions_taken"]["remaining_workflows"].extend([
                f"{dept_dir.name}/{w.name}" for w in dept_workflows
            ])
    
    # Add root level workflows
    root_workflows = list(N8N_WORKFLOWS_PATH.glob("*.json"))
    report["actions_taken"]["remaining_workflows"].extend([
        w.name for w in root_workflows
    ])
    
    report["statistics"]["total_after"] = len(report["actions_taken"]["remaining_workflows"])
    
    # Save report
    report_path = BASE_PATH / "workflow_cleanup_report.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nüìã Final Summary:")
    print(f"  ‚Ä¢ Total workflows: {report['statistics']['total_after']}")
    print(f"  ‚Ä¢ Organized by department: ‚úÖ")
    print(f"  ‚Ä¢ Data flow compliance: ‚úÖ")
    print(f"  ‚Ä¢ Report saved to: {report_path}")

def main():
    """Run the complete workflow cleanup and creation process"""
    print("üöÄ VividWalls MAS Workflow Cleanup & Creation")
    print("=" * 60)
    
    # Step 1: Archive old/redundant workflows
    archive_dir = archive_workflows()
    
    # Step 2: Consolidate duplicates
    consolidate_duplicates()
    
    # Step 3: Create missing workflows
    create_missing_workflows()
    
    # Step 4: Organize by department
    organize_workflows()
    
    # Step 5: Generate report
    generate_summary_report()
    
    print("\n‚úÖ Workflow cleanup and creation complete!")
    print(f"   Archived workflows saved to: {archive_dir}")
    
    return 0

if __name__ == "__main__":
    exit(main())